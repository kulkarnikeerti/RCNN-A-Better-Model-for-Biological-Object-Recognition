{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_imgs, train_lbls), (test_imgs, test_lbls) = tf.keras.datasets.cifar10.load_data()\n",
    "train_imgs = train_imgs/ 255.0\n",
    "test_imgs = test_imgs/ 255.0\n",
    "\n",
    "num_classes = 10\n",
    "train_lbls = np.squeeze(keras.utils.to_categorical(train_lbls, num_classes))\n",
    "test_lbls = np.squeeze(keras.utils.to_categorical(test_lbls, num_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dispaly of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = (train_imgs[400].squeeze())\n",
    "plt.imshow(img, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Addition of Gaussian Noise to the dataset and display of the noisy image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_gaussian_noise(images, std_dev):\n",
    "    n_imgs = images.shape[0]\n",
    "    n_rows = images.shape[1]\n",
    "    n_cols = images.shape[2]\n",
    "    n_chan = images.shape[3]\n",
    "    noise = np.random.normal(0.0 ,std_dev , (n_imgs, n_rows, n_cols, n_chan))\n",
    "    noisy_X = images + noise\n",
    "    noise_images = np.clip(noisy_X, 0., 1.)\n",
    "    return noise_images\n",
    "\n",
    "noise_test_imgs = add_gaussian_noise(test_imgs, 1)  #Depending on the level of noise, value is assigned\n",
    "\n",
    "noise_img = (noise_train_imgs[15].squeeze())\n",
    "plt.imshow(noise_img, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating model 'BF'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.InputLayer(input_shape= (32,32,3)),\n",
    "    tf.keras.layers.Conv2D(64,3, strides = (1, 1), padding='same', use_bias=True,activation = 'relu'),\n",
    "    tf.keras.layers.MaxPool2D((2, 2),(2, 2)),\n",
    "    tf.keras.layers.Conv2D(64, 3, strides = (1, 1), padding='same', use_bias=True,activation = 'relu'),\n",
    "    tf.keras.layers.GlobalMaxPooling2D(data_format='channels_last'),\n",
    "    tf.keras.layers.Dense(10, activation = 'sigmoid', kernel_regularizer=tf.keras.regularizers.l2(0.0005))\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing parameters for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras import callbacks\n",
    "\n",
    "def step_decay(epoch):\n",
    "    initial_lr=0.001\n",
    "    decay_rate = 0.01\n",
    "    decay_step = 30\n",
    "    lrate = initial_lr * decay_rate ** (epoch/decay_step)\n",
    "    print(\"learning_rate\")\n",
    "    print(lrate)\n",
    "    return lrate\n",
    "\n",
    "Adam = tf.keras.optimizers.Adam(lr=0.0)\n",
    "model.compile(loss='categorical_crossentropy', optimizer = Adam, metrics=['accuracy'])\n",
    "\n",
    "lrate = LearningRateScheduler(step_decay)\n",
    "callbacks_list = [lrate]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer = Adam, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#With Noise\n",
    "history_dict = dict()\n",
    "for i in range(1, 101):\n",
    "    noise_train_imgs = add_gaussian_noise(train_imgs, 1)  #noise is either 1 or 2, depending on the noise level\n",
    "    history_dict['epoch_%i' % i] = model.fit(noise_train_imgs, train_lbls, epochs=1, batch_size=128, validation_split=0.1, callbacks = callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history.history.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With noise\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "val_loss_list = []\n",
    "val_acc_list = []\n",
    "\n",
    "for i in range(1,101):\n",
    "    print(i)\n",
    "    val_loss = history_dict['epoch_%i' % i].history['val_loss']\n",
    "    val_acc = history_dict['epoch_%i' % i].history['val_accuracy']\n",
    "    acc = history_dict['epoch_%i' % i].history['accuracy']\n",
    "    loss = history_dict['epoch_%i' % i].history['loss']\n",
    "\n",
    "    train_loss_list.append(loss)\n",
    "    train_acc_list.append(acc)\n",
    "    val_loss_list.append(val_loss)\n",
    "    val_acc_list.append(val_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#With Noise\n",
    "model.evaluate(noise_test_imgs, test_lbls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 4\n",
    "plt.figure(figsize=(15,15))\n",
    "for i in range(n):\n",
    "    plt.subplot(1,n,i+1)\n",
    "    plt.imshow(test_imgs[i])\n",
    "    plt.title(\"Lable:{}\\nPredicted:{}\".format(test_lbls[i],np.argmax(predictions[i])))\n",
    "    plt.axis='off'\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the model and the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('Model_BF.h5') #Depending on the level of noise and the model, name is given"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_predictions = []\n",
    "ground_truth_values = []\n",
    "for i in range(len(test_imgs)):\n",
    "    model_predictions.append(np.argmax(predictions[i]))\n",
    "      ground_truth_values.append(np.argmax(test_lbls[i]))\n",
    "\n",
    "\n",
    "correct_prediction = [i if i==j else 0 for i, j in zip(model_predictions, ground_truth_values)]\n",
    "import pandas as pd\n",
    "prediction_df = pd.DataFrame(correct_prediction, columns=['result']).to_csv('BF_1_prediction.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
