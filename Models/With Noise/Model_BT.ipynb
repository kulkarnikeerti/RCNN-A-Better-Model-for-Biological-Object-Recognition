{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_imgs, train_lbls), (test_imgs, test_lbls) = tf.keras.datasets.cifar10.load_data()\n",
    "train_imgs = train_imgs/ 255.0\n",
    "test_imgs = test_imgs/ 255.0\n",
    "\n",
    "num_classes = 10\n",
    "train_lbls = np.squeeze(keras.utils.to_categorical(train_lbls, num_classes))\n",
    "test_lbls = np.squeeze(keras.utils.to_categorical(test_lbls, num_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dispaly of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = (train_imgs[400].squeeze())\n",
    "plt.imshow(img, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Addition of Gaussian Noise to the dataset and display of the noisy image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_gaussian_noise(images, std_dev):\n",
    "    n_imgs = images.shape[0]\n",
    "    n_rows = images.shape[1]\n",
    "    n_cols = images.shape[2]\n",
    "    n_chan = images.shape[3]\n",
    "    noise = np.random.normal(0.0 ,std_dev , (n_imgs, n_rows, n_cols, n_chan))\n",
    "    noisy_X = images + noise\n",
    "    noise_images = np.clip(noisy_X, 0., 1.)\n",
    "    return noise_images\n",
    "\n",
    "noise_test_imgs = add_gaussian_noise(test_imgs, 1)  #Depending on the level of noise, value is assigned\n",
    "\n",
    "noise_img = (noise_train_imgs[15].squeeze())\n",
    "plt.imshow(noise_img, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating model 'BT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend\n",
    "from tensorflow.keras.layers import Layer\n",
    "class LocalResponseNormalization(Layer):\n",
    "\n",
    "    def __init__(self, n=5, alpha=0.0001, beta=0.5, c=1, **kwargs):\n",
    "\n",
    "        self.n = n\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.c = c\n",
    "        super(LocalResponseNormalization, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.shape = input_shape\n",
    "        super(LocalResponseNormalization, self).build(input_shape)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        local_response = tf.nn.local_response_normalization(x, depth_radius = self.n, bias = self.c, alpha = self.alpha, beta = self.beta)\n",
    "        return local_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicLayers(object):\n",
    "\n",
    "    def __init__(self, filters, kernel_size):\n",
    "\n",
    "        self.b_conv = tf.keras.layers.Conv2D(filters, kernel_size, padding='same', use_bias=False)\n",
    "        self.t_conv = tf.keras.layers.Conv2DTranspose(filters, kernel_size, strides=(2,2), padding='same', use_bias=False)\n",
    "\n",
    "    def __call__(self, b_input=None, t_input=None):\n",
    "        conv_list = []\n",
    "\n",
    "        if b_input is not None:\n",
    "            conv_list.append(self.b_conv(b_input))\n",
    "          \n",
    "        if t_input is not None:\n",
    "            conv_list.append(self.t_conv(t_input))\n",
    "            \n",
    "        return tf.add_n(conv_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bt_net(input_tensor, classes, time_steps):\n",
    "\n",
    "        layers = [BLConvLayer(32, 3),\n",
    "            BLConvLayer(32, 3)]\n",
    "\n",
    "        n_layers = len(layers)\n",
    "        hidden_states = [[None for _ in range(n_layers)]\n",
    "                       for _ in range(time_steps)]\n",
    "        before_sigmoid = [None for _ in range(time_steps)]\n",
    "        outputs = [None for _ in range(time_steps)]\n",
    "\n",
    "        for t in range(time_steps):\n",
    "            for n, layer in enumerate(layers):\n",
    "\n",
    "                if n == 0:\n",
    "                    b_input = input_tensor \n",
    "                else:\n",
    "                    b_input = tf.keras.layers.MaxPool2D(pool_size=(2, 2))(hidden_states[t][n-1])\n",
    "                \n",
    "                if t!=0 and n==0:\n",
    "                    t_input = hidden_states[t-1][n+1]\n",
    "                else:\n",
    "                    t_input = None\n",
    "\n",
    "                x_tn = layer(b_input, t_input)\n",
    "                relu_output = tf.keras.layers.Activation('relu')(x_tn)\n",
    "                hidden_states[t][n] = LocalResponseNormalization()(relu_output)\n",
    "\n",
    "            x = tf.keras.layers.GlobalMaxPool2D()(hidden_states[t][-1])\n",
    "            before_sigmoid[t] = tf.keras.layers.Dense(classes, kernel_regularizer=tf.keras.regularizers.l2(0.0005))(x)\n",
    "            \n",
    "            if t > 0:\n",
    "                x = tf.add_n(before_sigmoid[:t+1])\n",
    "            else:\n",
    "                x = before_sigmoid[t]\n",
    "            outputs[t] = tf.keras.layers.Activation('sigmoid')(x)\n",
    "\n",
    "        model = tf.keras.Model(inputs=input_tensor,outputs=outputs)    \n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.layers.Input(shape = (32,32,3))\n",
    "model = bt_net(inputs, 10, 4)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing parameters for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras import callbacks\n",
    "\n",
    "def step_decay(epoch):\n",
    "    initial_lr=0.001\n",
    "    decay_rate = 0.01\n",
    "    decay_step = 30\n",
    "    lrate = initial_lr * decay_rate ** (epoch/decay_step)\n",
    "    print(\"learning_rate\")\n",
    "    print(lrate)\n",
    "    return lrate\n",
    "\n",
    "Adam = tf.keras.optimizers.Adam(lr=0.0)\n",
    "model.compile(loss='categorical_crossentropy', optimizer = Adam, metrics=['accuracy'])\n",
    "\n",
    "lrate = LearningRateScheduler(step_decay)\n",
    "callbacks_list = [lrate]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer = Adam, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is used with the dataset where noise has been added.\n",
    "history_dict = dict()\n",
    "for i in range(1, 101):\n",
    "    noise_train_imgs = add_gaussian_noise(train_imgs, 1)  #noise is either 1 or 2, depending on the noise level\n",
    "    history_dict['epoch_%i' % i] = model.fit([noise_train_imgs, noise_train_imgs, noise_train_imgs, noise_train_imgs], [train_lbls, train_lbls, train_lbls, train_lbls], epochs=1, batch_size=128, validation_split=0.1, callbacks = callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history.history.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With noise\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "val_loss_list = []\n",
    "val_acc_list = []\n",
    "\n",
    "for i in range(1,101):\n",
    "    print(i)\n",
    "    val_loss = history_dict['epoch_%i' % i].history['val_loss']\n",
    "    val_acc = history_dict['epoch_%i' % i].history['val_accuracy']\n",
    "    acc = history_dict['epoch_%i' % i].history['accuracy']\n",
    "    loss = history_dict['epoch_%i' % i].history['loss']\n",
    "\n",
    "    train_loss_list.append(loss)\n",
    "    train_acc_list.append(acc)\n",
    "    val_loss_list.append(val_loss)\n",
    "    val_acc_list.append(val_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#With Noise\n",
    "model.evaluate([noise_test_imgs,noise_test_imgs,noise_test_imgs,noise_test_imgs], [test_lbls,test_lbls,test_lbls,test_lbls])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 4\n",
    "plt.figure(figsize=(15,15))\n",
    "for i in range(n):\n",
    "    plt.subplot(1,n,i+1)\n",
    "    plt.imshow(test_imgs[i])\n",
    "    plt.title(\"Lable:{}\\nPredicted:{}\".format(test_lbls[i],np.argmax(predictions[i])))\n",
    "    plt.axis='off'\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the model and the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('Model_BT.h5') #Depending on the level of noise, name is given"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_predictions = []\n",
    "ground_truth_values = []\n",
    "for i in range(len(test_imgs)):\n",
    "    model_predictions.append(np.argmax(predictions[i]))\n",
    "      ground_truth_values.append(np.argmax(test_lbls[i]))\n",
    "\n",
    "\n",
    "correct_prediction = [i if i==j else 0 for i, j in zip(model_predictions, ground_truth_values)]\n",
    "import pandas as pd\n",
    "prediction_df = pd.DataFrame(correct_prediction, columns=['result']).to_csv('BLT_1_prediction.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
